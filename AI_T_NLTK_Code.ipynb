{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPr5e-Y6Ydn_",
        "outputId": "3b313130-87a8-4d4c-acd8-7621b61b0ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from NLTK) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from NLTK) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from NLTK) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from NLTK) (4.67.3)\n"
          ]
        }
      ],
      "source": [
        "### AI_Session , Date: 11/02/2026 day : Wednesday\n",
        "### Topic:\n",
        "\n",
        "\n",
        "\n",
        "!pip install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews,stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "tTxR7HDHi2py"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('movie_reviews')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsuePDrBjqoQ",
        "outputId": "de0543ff-574f-48ea-8644-c02f4ca90729"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"NLTK movies reviews sentiments Analysis\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxse3jCnkFjw",
        "outputId": "8ca9a1cf-e4a4-416c-d941-56dc5b82c4ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NLTK movies reviews sentiments Analysis\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n [1] Dataset Overview\")\n",
        "print(f\" Total Documents : {len(movie_reviews.fileids())}\")\n",
        "print(f\"Categories : {movie_reviews.categories()}\")\n",
        "print(f\"Positive reviews :{len(movie_reviews.fileids('pos'))}\")\n",
        "print(f\"Negative reviews :{len(movie_reviews.fileids('neg'))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSSUagV0kaHZ",
        "outputId": "36b9865f-9b89-4cd3-b5d6-ba565b0cd08a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [1] Dataset Overview\n",
            " Total Documents : 2000\n",
            "Categories : ['neg', 'pos']\n",
            "Positive reviews :1000\n",
            "Negative reviews :1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  sample reviews\n",
        "sample_file = movie_reviews.fileids()[0]\n",
        "print(f\"\\n Sample file : {sample_file}\")\n",
        "print(f\"First 200 chars : {movie_reviews.raw(sample_file)[:200]}.....\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKu5qUQ8nVQy",
        "outputId": "824e0467-439e-41c5-ac04-2da613484e1d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample file : neg/cv000_29416.txt\n",
            "First 200 chars : plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
            "w.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string # Import the string module\n",
        "\n",
        "# text perprocessing pipeline\n",
        "def perprocess_text(text):\n",
        "  # trokenize,lowercase nad remove stopwords/punctuation\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  # Corrected 'isaplha()' to 'isalpha()' and added punctuation removal\n",
        "  cleanes =[word for word in tokens if word.isalpha() and word not in stop_words and word not in string.punctuation]\n",
        "  return cleanes"
      ],
      "metadata": {
        "id": "Lq8Zr0Z0oBHa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "random_review  = random.choice (movie_reviews.fileids())\n",
        "raw_text = movie_reviews.raw(random_review)\n",
        "cleaned_tokens = preprocess_text(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFIAFuOLpag-",
        "outputId": "095ec69d-1a6e-4e30-bd0e-9ba4ab867a8a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n [2] text preprocessing example : \")\n",
        "\n",
        "print(f\"Original tokens (first 15) : {word_tokenize(raw_text.lower())[:15]}\")\n",
        "\n",
        "print(f\"Cleaned tokens (first 15): {cleaned_tokens[:15]}\")\n",
        "\n",
        "print(f\"Tokens before cleaning: {len(word_tokenize(raw_text.lower()))}\")\n",
        "\n",
        "print(f\"Tokens after cleaning: {len(cleaned_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP-i9OLVqr9D",
        "outputId": "f96bbf6e-6656-451a-b0c8-35817407c48a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [2] text preprocessing example : \n",
            "Original tokens (first 15) : ['they', 'should', 'have', 'stuck', 'to', 'the', 'promise', 'emblazoned', 'on', 'the', 'original', 'movie', \"'s\", 'poster', ':']\n",
            "Cleaned tokens (first 15): ['stuck', 'promise', 'emblazoned', 'original', 'movie', 'poster', 'sequel', 'scary', 'movie', 'nowhere', 'near', 'funny', 'predecessor', 'wayans', 'brothers']\n",
            "Tokens before cleaning: 491\n",
            "Tokens after cleaning: 217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihO5bXsarcqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}